{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "WebScraping_Public_Fast_Version_2011 .ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "uzOCg6nNMoIK"
      },
      "source": [
        "from bs4 import BeautifulSoup,SoupStrainer\n",
        "import requests\n",
        "import sys\n",
        "from bs4.element import Comment\n",
        "from dateutil.parser import parse\n",
        "from trash import isTrash\n",
        "import datetime\n",
        "import time\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import concurrent.futures"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FcrDR8y8M_Nz"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cvWjLXaPNGax"
      },
      "source": [
        "!ls '/content/drive/My Drive/Capstone Shared Docs/result'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4JZAt45bNLUs"
      },
      "source": [
        "data = pd.read_stata(\"drive/My Drive/Capstone Shared Docs/data/all_public_firms.dta\")\n",
        "data['ipoyear'] = pd.DatetimeIndex(data['ipodate']).year"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WVcK5tjRNQEZ"
      },
      "source": [
        "\n",
        "def hold_your_horses(seconds=5):\n",
        "    time.sleep(seconds)\n",
        "\n",
        "\n",
        "def target_date(website_v,year_v):\n",
        "    \"\"\"\n",
        "    Will reuturn the first timestamp from\n",
        "    the specified year\n",
        "    \"\"\"\n",
        "    # I changed this function to return the first available snapshot timestamp\n",
        "    try:\n",
        "        hold_your_horses()\n",
        "        r = requests.get(\"https://web.archive.org/__wb/calendarcaptures?url=\"+website_v+\"&selected_year=\"+str(year_v),timeout=5).json()\n",
        "        for m in range(12):\n",
        "            for w in range(4):\n",
        "                for t in range(7):\n",
        "                    value = r[m][w][t]\n",
        "                    if value is None:\n",
        "                        continue\n",
        "                    if 'ts' in value.keys():\n",
        "                        timestamp = value['ts'][0]\n",
        "                        if timestamp is not None:\n",
        "                            return timestamp\n",
        "                    else:\n",
        "                        continue\n",
        "            # except:\n",
        "            # \treturn None\n",
        "        return None\n",
        "    except :\n",
        "        error_msg = sys.exc_info()[1]\n",
        "        log_error(\"target_date\", error_msg)\n",
        "        return None\n",
        "\n",
        "\n",
        "\n",
        "def tag_visible(element):\n",
        "    if element.parent.name in ['style', 'script', 'head', 'header', 'footer', 'title', 'meta', '[document]']:\n",
        "        return False\n",
        "    if isinstance(element, Comment):\n",
        "        return False\n",
        "    return True\n",
        "\n",
        "\n",
        "def remove_header_and_footer(soup):\n",
        "    for e in soup.findAll(\"div\", {\"class\": [\"headers\", \"header\", \"footer\", \"footers\"]}):\n",
        "        e.extract()\n",
        "\n",
        "\n",
        "def text_from_html(soup):  # take in a bs object, not just the link name, need process before hand\n",
        "\n",
        "    # I added this function to remove header and footer from the html\n",
        "    remove_header_and_footer(soup)\n",
        "\n",
        "    texts = soup.findAll(text=True)\n",
        "    visible_texts = filter(tag_visible, texts)\n",
        "    raw = u\" \".join(t.strip() for t in visible_texts)\n",
        "    return str(raw.replace('\\t', ' ').replace('\\\\n', ' ').replace('\\n', ' ').strip().encode('ascii', 'ignore'))\n",
        "\n",
        "def page2text(soup):\n",
        "    ##Function for scrape text from html\n",
        "    #try:\n",
        "    content = text_from_html(soup)\n",
        "    if content is None or isTrash(content):\n",
        "        return \"\"\n",
        "    else:\n",
        "        return content\n",
        "\n",
        "\n",
        "def homePage(url,timestamp):\n",
        "    if timestamp is None:\n",
        "        return None, None\n",
        "    url = \"https://web.archive.org/web/\"+str(timestamp)+\"/\"+url+\"/\"\n",
        "    soup = getSoup(url)\n",
        "    return soup,url\n",
        "\n",
        "def getSoup(url):\n",
        "    try:\n",
        "        hold_your_horses()\n",
        "        resp = requests.get(url,timeout=5) # originally, timeout = 3, which may cause the occasionally error in request\n",
        "        return BeautifulSoup(resp.content, 'html.parser', from_encoding='iso-8859-1') #from_encoding='iso-8859-1'\n",
        "    except:\n",
        "        error_msg = sys.exc_info()[1]\n",
        "        log_error(\"getSoup\", error_msg)\n",
        "        return None\n",
        "\n",
        "\n",
        "def findPages(website_v, soup, timestamp):\n",
        "    toFetch = soup.find_all('a', href=True)\n",
        "    toFetch = [link for link in toFetch if checkLink(link, website_v)]\n",
        "\n",
        "    return [getFullLink(link) for link in toFetch if checkDate(link, str(timestamp))]\n",
        "\n",
        "\n",
        "def checkLink(href, website_v):\n",
        "    # I added this function to remove links that are not useful (only keep link that belongs to the same website)\n",
        "    href = href[\"href\"]\n",
        "    invalid_href = [\"#\"]\n",
        "    return (href not in invalid_href) and (website_v in href)\n",
        "\n",
        "\n",
        "def checkDate(href, timestamp):\n",
        "    href = href['href']\n",
        "    try:\n",
        "        linkTime = href[href.find('/web/') + 5:href.find('/web/') + 5 + 14]\n",
        "        ogTime = datetime.datetime.strptime(timestamp, \"%Y%m%d%H%M%S\")\n",
        "        linkTime = datetime.datetime.strptime(linkTime, \"%Y%m%d%H%M%S\")\n",
        "        if abs((linkTime - ogTime).days) < 365:\n",
        "            return True\n",
        "        else:\n",
        "            return False\n",
        "    except:\n",
        "        error_msg = sys.exc_info()[1]\n",
        "        log_error(\"checkDate\", error_msg)\n",
        "        return False\n",
        "\n",
        "\n",
        "def getFullLink(href):\n",
        "    href = href['href']\n",
        "    if href.startswith('http') or href.startswith('https'):\n",
        "        return href\n",
        "    else:\n",
        "        return \"https://web.archive.org\" + href\n",
        "\n",
        "\n",
        "def add_to_visited(visited, url, website_v):\n",
        "    where_is = url.find(website_v)\n",
        "    before = url[:where_is]\n",
        "    after = url[where_is + len(website_v):]\n",
        "\n",
        "    if before.endswith(\"http://\"):\n",
        "        before = before[:-7]\n",
        "\n",
        "    if before.endswith(\"https://\"):\n",
        "        before = before[:-8]\n",
        "\n",
        "    visited.append(before + website_v + after)\n",
        "    visited.append(before + \"http://\" + website_v + after)\n",
        "    visited.append(before + \"https://\" + website_v + after)\n",
        "\n",
        "    # question: but why do we need these three forms of links?\n",
        "    # answer: to avoid the case of different url but actually the same domain\n",
        "\n",
        "    return visited\n",
        "\n",
        "def add_to_visited2(visited, url, website_v):\n",
        "    '''\n",
        "    this new version of visited, only keep the page domain as an \"id\"\n",
        "    and get rid of the time machine part in the real domain stored in time machine\n",
        "    '''\n",
        "    where_is = url.find(website_v)\n",
        "    after = url[where_is+len(website_v):]\n",
        "    visited.append(website_v+after)\n",
        "\n",
        "    return visited\n",
        "\n",
        "def getdomain(page,website_v):\n",
        "    where_is = page.find(website_v)\n",
        "    after = page[where_is+len(website_v):]\n",
        "    return website_v+after\n",
        "\n",
        "\n",
        "def getText(website_v, year_v, company_id): #nclude Id of company to include this in the output data, that is going to be a dict\n",
        "    # this is the homepage object\n",
        "    print(website_v, year_v)\n",
        "    visited = []\n",
        "    timestamp = target_date(website_v, year_v)\n",
        "    soup, home_page_url = homePage(website_v, timestamp)\n",
        "\n",
        "    if soup is None:  # should we break the function from here?\n",
        "        log_error(\"soup is None\")\n",
        "        return None, None\n",
        "\n",
        "    visited = add_to_visited2(visited, home_page_url, website_v)\n",
        "\n",
        "    # this is the list of links from the homepage\n",
        "    toFetch = findPages(website_v, soup, timestamp)\n",
        "    # this is the text form the homepage\n",
        "    # at least we will have information form the homepage\n",
        "    text = page2text(soup)\n",
        "\n",
        "    # to check duplicated\n",
        "    text_list = []  # This is new\n",
        "    text_list.append(text)  # This is also new\n",
        "    # looping links and get texts\n",
        "    \n",
        "    for page in toFetch:  ###\n",
        "        # the if prevents re-entering the link\n",
        "        # (!!but we find that some page links inside the homepage actually\n",
        "        # have the same domain but different timestep -> this if sentence is not effective)\n",
        "        \n",
        "        if getdomain(page, website_v) not in visited:\n",
        "\n",
        "            if '.pdf' in page:\n",
        "                continue\n",
        "            \n",
        "            else:\n",
        "                soup = getSoup(page)\n",
        "            \n",
        "            if soup is None:\n",
        "                continue\n",
        "                \n",
        "            text_list.append(page2text(soup)) #Store in list\n",
        "            \n",
        "            visited = add_to_visited2(visited, page, website_v)\n",
        "            \n",
        "            \n",
        "    ID = [company_id]*len(visited)\n",
        "    YR = [year_v]*len(visited)\n",
        "    lastpath = [os.path.basename(x) for x in visited]\n",
        "    \n",
        "    \n",
        "    dict_text = {'ID': ID ,'Year': YR, 'Visited_link': visited,'Name_Path':lastpath , 'Text': text_list}\n",
        "\n",
        "    #text_list =   # We deactivate this function to keep track of the links visited\n",
        "                 \n",
        "    text = ' ***///*** '.join(list(dict.fromkeys(text_list)))  # Store in text only non reapeted text\n",
        "    \n",
        "    print(f\"Visited {len(visited)} links\")\n",
        "    \n",
        "    text = text.strip()\n",
        "       \n",
        "    # wangzhi: to check the duplicated content among all abstracted text from all links\n",
        "    if len(text) > 50:  # 1000:\n",
        "        return text , dict_text\n",
        "    else:\n",
        "        log_error(\"Text is to short\")\n",
        "        return None, None\n",
        "\n",
        "\n",
        "# log functions\n",
        "def initialize_log():\n",
        "    log = pd.DataFrame({\"id\":[],\"website\":[],\"year\":[],\"timestamp\":[],\"error\":[],\"detailed_error\":[]})\n",
        "    return log\n",
        "\n",
        "def log_error(error,detailed_error=\"\"):\n",
        "    global log, year, website , timestamp, company_id\n",
        "    log = log.append({\"id\":company_id, \"website\":website,\"year\":year,\"timestamp\": datetime.datetime.now(),\"error\":error,\n",
        "                      \"detailed_error\":detailed_error}, ignore_index=True)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qh7786thPQli"
      },
      "source": [
        "def single_mining (X):\n",
        "    \n",
        "    text, dict_data_frame = getText(X[0], X[2], X[1])\n",
        "    \n",
        "    if text != None:\n",
        "        filename = X[3] + \"/\" + str(X[1]) + \"_\" + str(X[2]) + '.txt'\n",
        "        f = open(filename, \"w+\")\n",
        "        f.write(text)\n",
        "        f.close()    \n",
        "  \n",
        "    comp_dataframe = pd.DataFrame.from_dict(dict_data_frame)\n",
        "    \n",
        "    comp_dataframe.to_csv(X[3]+\"/Text_data_frame.csv\",mode = 'a',index = False, header= False)\n",
        "    \n",
        "         \n",
        "    del comp_dataframe,  filename "
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CGU-PNPsPQwq"
      },
      "source": [
        "MAX_THREADS = 30\n",
        "    \n",
        "def all_mining_fast(List_info):\n",
        "    threads = min(MAX_THREADS, len(List_info))\n",
        "    \n",
        "    with concurrent.futures.ThreadPoolExecutor(max_workers=threads) as executor:\n",
        "        executor.map(single_mining, List_info)"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Kit8-bNNgg0"
      },
      "source": [
        "path = os.getcwd() + '/drive/My Drive/Capstone Shared Docs/result/Colab Nov2 Public 2011'"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2nzxeB5fNyT6"
      },
      "source": [
        "if not os.path.exists(path):\n",
        "    os.makedirs(path)"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LBZcECoMOB4q"
      },
      "source": [
        "df_public = data[(data.ipoyear <= 2011) | (pd.isna(data.ipoyear)) ]\n",
        "log = initialize_log()\n",
        "dict_ini = {'ID': [],'Year': [], 'Visited_link': [], 'Name_Path':[], 'Text': []}\n",
        "text_data_frame = pd.DataFrame(dict_ini)\n",
        "text_data_frame.to_csv(path+'/Text_data_frame.csv', index=False)"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D5Z_9xbVOd6S"
      },
      "source": [
        "List_info = []\n",
        "year = 2012\n",
        "\n",
        "for index, row in df_public.iterrows():\n",
        "    website = row.websiteaddress  \n",
        "    company_id = row.ÿþmark\n",
        "    X = [website, company_id, year, path]\n",
        "    List_info.append(X)"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cq-5ETgiPAHr"
      },
      "source": [
        "%%time\n",
        "all_mining_fast(List_info)\n",
        "log.to_csv(path+\"/error_log.csv\",index=False)\n",
        "\n",
        "print(\"Done!\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}