{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "preprocessing.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zA104p08u80r"
      },
      "source": [
        "# Text preprocessing for strategy score"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oz4o3CsZu1u5"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pathlib\n",
        "import re\n",
        "import concurrent.futures"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hbn9QCmyvHvV",
        "outputId": "e02ff04f-41ff-4bff-ace1-51dab28afc81",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JnaBzBOmwpSt",
        "outputId": "743854b1-b806-4953-eda3-11b8297e2436",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!pip install gcld3"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting gcld3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/61/63/0f5816f6c60fce2f5b2d6106ad1d3a50fc785ac68b1520a78c80b4b84fb7/gcld3-3.0.13-cp36-cp36m-manylinux2010_x86_64.whl (3.8MB)\n",
            "\u001b[K     |████████████████████████████████| 3.8MB 5.7MB/s \n",
            "\u001b[?25hInstalling collected packages: gcld3\n",
            "Successfully installed gcld3-3.0.13\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZjL6ZxqWwmfE"
      },
      "source": [
        "import gcld3\n",
        "detector = gcld3.NNetLanguageIdentifier(min_num_bytes=0, max_num_bytes=10000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FK4XUhCEvJki",
        "outputId": "310de8fe-4c85-4013-d116-e5bce25b30b3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CAFD1QNb7XMT"
      },
      "source": [
        "MAX_THREADS = 30"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jvcw03xCvMCJ"
      },
      "source": [
        "df_startups = pd.read_stata(\"drive/My Drive/Capstone Shared Docs/data/all_deals.dta\")\n",
        "df_startups_unique = df_startups.drop_duplicates([\"portfoliocompanyid\"],keep=\"first\")\n",
        "\n",
        "df_public = pd.read_stata(\"drive/My Drive/Capstone Shared Docs/data/all_public_firms.dta\")\n",
        "df_public_unique = df_public.drop_duplicates([\"ÿþmark\"],keep=\"first\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L-xRT_ZFvSfC",
        "outputId": "d7c61de9-20c9-4c2d-d67b-8ba19f4b7fe3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "drive_folder= \"drive/My Drive/Capstone Shared Docs/result\"\n",
        "txt_folder = drive_folder + \"/Txt Files Public and Startups\"\n",
        "txt_folder_2 = txt_folder.replace(\" \",\"\\ \")\n",
        "!ls $txt_folder_2"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "'Public Companies'   Startups\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iQCLhpxLwBjT"
      },
      "source": [
        "def get_text_paths(p):\n",
        "  return list(p.glob('*.txt'))\n",
        "\n",
        "def read_text_file(p):\n",
        "  n = p.name.split(\"_\")[0]\n",
        "  file = open(p)\n",
        "  # Remove first two and last characters, because of ascii encoding\n",
        "  text = file.read().replace(\"***///***\",\"\\n\")[2:-1]\n",
        "  # remove \"\"\n",
        "  text_list = text.split(\" \")\n",
        "  text_list = [x for x in text_list if x!=\"\"]\n",
        "  text = \" \".join(text_list)\n",
        "  file.close()\n",
        "  return n,text\n",
        "\n",
        "def get_text(files_list):\n",
        "  threads = min(MAX_THREADS, len(files_list))\n",
        "  company_text = {}\n",
        "  data_futures = []\n",
        "  with concurrent.futures.ThreadPoolExecutor(max_workers = threads) as thread_pool_executor:\n",
        "    data_futures = [thread_pool_executor.submit(read_text_file, p) for p in files_list]\n",
        "  for data_future in concurrent.futures.as_completed(data_futures):\n",
        "      future = data_future.result()\n",
        "      company_text[future[0]] = future[1]\n",
        "  return company_text\n",
        "    \n",
        "\n",
        "def create_data_frame(company_text, company_type):\n",
        "  company_frame = pd.DataFrame.from_dict(company_text, orient='index')\n",
        "  company_frame.columns = ['text']\n",
        "  # company_frame['companyid'] = company_frame.index\n",
        "  # company_frame.index = np.arange(0,len(company_frame.companyid))\n",
        "  company_frame = company_frame.assign(comp_type=company_type)\n",
        "  stop_words_l=stopwords.words('english')\n",
        "  company_frame['text_cleaned']=company_frame.text.apply(lambda x: \" \".join(re.sub(r'[^a-zA-Z]',' ',w).lower() for w in x.split() if re.sub(r'[^a-zA-Z]',' ',w).lower() not in stop_words_l) )\n",
        "  company_frame['text_cleaned_with_sw']=company_frame.text.apply(lambda x: \" \".join(re.sub(r'[^a-zA-Z]',' ',w).lower() for w in x.split() ))\n",
        "  company_frame[\"compid\"] = company_frame.index\n",
        "  company_frame.reset_index(inplace=True,drop=True)\n",
        "\n",
        "  return company_frame\n",
        "\n",
        "def get_path(c_type,year):\n",
        "  if c_type==\"S\":\n",
        "    folder_name = txt_folder + f\"/Startups/Startup {year}\"\n",
        "  else:\n",
        "    folder_name = txt_folder + f\"/Public Companies/Public {year}\"\n",
        "  return pathlib.Path(folder_name)\n",
        "\n",
        "\n",
        "def build_data_frame(c_type,year):\n",
        "  path = get_path(c_type, year)\n",
        "  files_path = get_text_paths(path)\n",
        "  print(f\"{c_type} {year} text files: \"+str(len(files_path)))\n",
        "  files = get_text(files_path)\n",
        "  df = create_data_frame(files,c_type)\n",
        "  return df\n",
        "\n",
        "def merge_and_resolve_language(df1,df2):\n",
        "  all_f = pd.concat([df1, df2]).reset_index(drop=True)\n",
        "  \n",
        "  # all_f.drop_duplicates(subset=\"text\", keep=\"first\",inplace=True)\n",
        "\n",
        "  lan = []\n",
        "  rel_lan = []\n",
        "  for i,r in all_f.iterrows():\n",
        "    t =r.text_cleaned\n",
        "    t = str(t)\n",
        "    \n",
        "    result = detector.FindLanguage(text=t)\n",
        "    lan.append(result.language)\n",
        "    rel_lan.append(result.is_reliable)\n",
        "  all_f['language'] = lan\n",
        "  all_f['Rela_language'] = rel_lan\n",
        "\n",
        "  # all_f_en = all_f[(all_f.language=='en') ].copy() #| ( (all_f.language!='en') & (all_f.language==False) )\n",
        "  all_f_en = all_f\n",
        "  # all_f_en.reset_index(drop=True,inplace=True)\n",
        "\n",
        "  all_f_en[\"text_len\"] = all_f_en.text_cleaned.apply(lambda x: len(x))\n",
        "  all_f_en[\"words_len\"] = all_f_en.text_cleaned.apply(lambda x: len(x.split()))\n",
        "\n",
        "  return all_f_en"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R23JfnwVw0FH"
      },
      "source": [
        "def add_informative_columns(all_f, year):\n",
        "  s_path = pathlib.Path(txt_folder +f\"/Startups/Startup {year}\")\n",
        "  p_path = pathlib.Path(txt_folder +f\"/Public Companies/Public {year}\")\n",
        "\n",
        "  startup_code_to_name = {}\n",
        "  startup_code_to_website = {}\n",
        "  public_code_to_name = {}\n",
        "  public_code_to_website = {}\n",
        "\n",
        "  for p in get_text_paths(s_path):\n",
        "    n = int(p.name.split(\"_\")[0])\n",
        "    startup_code_to_name[n] = df_startups_unique[df_startups_unique.portfoliocompanyid==n].iloc[0].portfoliocompany\n",
        "    startup_code_to_website[n] = df_startups_unique[df_startups_unique.portfoliocompanyid==n].iloc[0].website\n",
        "\n",
        "  for p in get_text_paths(p_path):\n",
        "    n = str(p.name.split(\"_\")[0])\n",
        "    public_code_to_name[n] = df_public_unique[df_public_unique[\"ÿþmark\"]==n].iloc[0].companyname\n",
        "    public_code_to_website[n] = df_public_unique[df_public_unique[\"ÿþmark\"]==n].iloc[0].websiteaddress\n",
        "\n",
        "  startup_code_to_info = df_startups_unique.set_index(\"portfoliocompanyid\").background.to_dict()\n",
        "  public_code_to_info = df_public_unique.set_index(\"ÿþmark\").descriptionandhistory.to_dict()\n",
        "\n",
        "  all_f.loc[all_f.comp_type==\"S\",\"website\"] = all_f[all_f.comp_type==\"S\"].compid.astype(int).map(startup_code_to_website)\n",
        "  all_f.loc[all_f.comp_type==\"S\",\"name\"] = all_f[all_f.comp_type==\"S\"].compid.astype(int).map(startup_code_to_name)\n",
        "  all_f.loc[all_f.comp_type==\"S\",\"info\"] = all_f[all_f.comp_type==\"S\"].compid.astype(int).map(startup_code_to_info)\n",
        "\n",
        "  all_f.loc[all_f.comp_type==\"P\",\"website\"] = all_f[all_f.comp_type==\"P\"].compid.astype(str).map(public_code_to_website)\n",
        "  all_f.loc[all_f.comp_type==\"P\",\"name\"] = all_f[all_f.comp_type==\"P\"].compid.astype(str).map(public_code_to_name)\n",
        "  all_f.loc[all_f.comp_type==\"P\",\"info\"] = all_f[all_f.comp_type==\"P\"].compid.astype(str).map(public_code_to_info)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_qphddlZw2br"
      },
      "source": [
        "def preprocess_year(year):\n",
        "  df_1 = build_data_frame(\"S\",year)\n",
        "  df_2 = build_data_frame(\"P\",year)\n",
        "  df_3 = merge_and_resolve_language(df_1,df_2)\n",
        "  add_informative_columns(df_3,year)\n",
        "  return df_3"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cuLQZoER3g_o",
        "outputId": "c97c8211-dfcc-4b23-8190-84a27947058f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Run this to create csv for all years\n",
        "for i in range(2011,2020):\n",
        "  data = preprocess_year(i)\n",
        "  data.to_csv(drive_folder+f\"/complete_df/year_{i}.csv\",index=False)\n",
        "  print(f\"Year {i} completed!\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "S 2011 text files: 539\n",
            "P 2011 text files: 8754\n",
            "Year 2011 completed!\n",
            "S 2012 text files: 769\n",
            "P 2012 text files: 9204\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gnd3lm-a5-4v"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}